<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Wajah dan Mata + Iris</title>
  <style>
    button { margin:5px; padding:10px 20px; font-size:16px; }
    body { text-align:center; font-family: sans-serif; background:#f4f4f4; }
    video, canvas { width:90%; max-width:480px; margin:10px; border-radius:10px; }
  </style>
</head>
<body>
  <h2>Deteksi Wajah, Mata, dan Iris</h2>
  <button id="startBtn">Start Camera</button>
  <button id="switchBtn">Switch Camera</button>
  <button id="captureBtn">Capture</button>
  <br>
  <video id="video" autoplay playsinline></video>
  <br>
  <canvas id="canvasOutput"></canvas>

  <script async src="https://docs.opencv.org/4.x/opencv.js" onload="onOpenCvReady();"></script>

  <script>
    let video = document.getElementById('video');
    let canvas = document.getElementById('canvasOutput');
    let ctx = canvas.getContext('2d');
    let faceCascade, eyeCascade;
    
    let currentFacingMode = "environment";
    let stream = null;

    function onOpenCvReady() {
      console.log("OpenCV.js loaded");
      document.getElementById('startBtn').disabled = false;
    }

    async function startCamera(facingMode = "environment") {
      try {
        if (stream) stream.getTracks().forEach(track => track.stop());
        stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: facingMode },
          audio: false
        });
        video.srcObject = stream;
        currentFacingMode = facingMode;
        console.log("Camera started:", facingMode);
      } catch (err) {
        console.error("Camera error:", err);
        alert("Cannot access camera: " + err.message);
      }
    }

    document.getElementById('startBtn').onclick = async () => {
      await loadCascades();
      startCamera();
    };

    document.getElementById('switchBtn').onclick = () => {
      const newMode = currentFacingMode === "environment" ? "user" : "environment";
      startCamera(newMode);
    };

    async function loadCascades() {
      async function loadCascade(name) {
        const url = "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/" + name;
        const response = await fetch(url);
        const data = new Uint8Array(await response.arrayBuffer());
        cv.FS_createDataFile('/', name, data, true, false);
        const classifier = new cv.CascadeClassifier();
        classifier.load(name);
        return classifier;
      }

      faceCascade = await loadCascade("haarcascade_frontalface_default.xml");
      eyeCascade = await loadCascade("haarcascade_eye.xml");
      console.log("Cascades loaded");
    }

    document.getElementById('captureBtn').onclick = function() {
      if (video.videoWidth === 0) return alert("Camera not started yet!");
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0);
      detectFacesAndEyes();
    };

    // --- FACE + EYE + IRIS DETECTION ---
    function detectFacesAndEyes() {
      let src = cv.imread(canvas);
      let gray = new cv.Mat();
      cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);

      let faces = new cv.RectVector();
      faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0);

      for (let i = 0; i < faces.size(); ++i) {
        let f = faces.get(i);

        // Face bounding box
        cv.rectangle(src,
          new cv.Point(f.x, f.y),
          new cv.Point(f.x + f.width, f.y + f.height), [255, 0, 0, 255], 2);

        // Restrict to upper half of face
        let upperHalfRect = new cv.Rect(f.x, f.y, f.width, Math.floor(f.height / 2));
        let faceROI = gray.roi(upperHalfRect);

        let eyes = new cv.RectVector();
        eyeCascade.detectMultiScale(faceROI, eyes, 1.1, 5, 0, new cv.Size(20, 20));

        let eyeRects = [];
        for (let j = 0; j < eyes.size(); ++j) {
          let e = eyes.get(j);
          eyeRects.push({ x: e.x, y: e.y, w: e.width, h: e.height });
        }

        // Sort and filter
        eyeRects.sort((a, b) => a.x - b.x);
        if (eyeRects.length > 2) {
          let avgY = eyeRects.reduce((s, e) => s + e.y, 0) / eyeRects.length;
          eyeRects = eyeRects.filter(e => Math.abs(e.y - avgY) < f.height / 6).slice(0, 2);
        } else {
          eyeRects = eyeRects.slice(0, 2);
        }

        // Draw eyes and detect iris
        for (let e of eyeRects) {
          let ex = upperHalfRect.x + e.x;
          let ey = upperHalfRect.y + e.y;

          cv.rectangle(src,
            new cv.Point(ex, ey),
            new cv.Point(ex + e.w, ey + e.h), [0, 255, 0, 255], 2);

          // --- Hough transform for iris detection ---
          let eyeROI = gray.roi(new cv.Rect(ex, ey, e.w, e.h));
          let eyeBlur = new cv.Mat();
          cv.medianBlur(eyeROI, eyeBlur, 5);
          let circles = new cv.Mat();

          // detect small dark circles (iris)
          cv.HoughCircles(eyeBlur, circles, cv.HOUGH_GRADIENT, 1,
                          eyeBlur.rows / 8, 200, 15, e.w / 8, e.w / 3);

          for (let k = 0; k < circles.cols; ++k) {
            let x = circles.data32F[k * 3];
            let y = circles.data32F[k * 3 + 1];
            let center = new cv.Point(ex + x, ey + y);
            cv.circle(src, center, 2, [0, 0, 255, 255], 3); // red dot
          }

          eyeROI.delete();
          eyeBlur.delete();
          circles.delete();
        }

        faceROI.delete();
        eyes.delete();
      }

      cv.imshow(canvas, src);
      src.delete(); gray.delete(); faces.delete();
    }
  </script>
</body>
</html>
